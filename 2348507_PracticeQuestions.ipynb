{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAl35MP6fV1HY5cOsxCjg/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/badrinarayanan02/deep_learning/blob/main/2348507_PracticeQuestions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practice Questions**"
      ],
      "metadata": {
        "id": "o5v8JU_47_Q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Assume we are attempting to train a back-propagation neural network with two hidden layers and a single node in each layer. Let us assume that the activation function is sigmoid function and all the weights and biases are initially set to w=1, b= -0.5, respectively. Compute the forward-pass output for input 0.5. Let the input 0.5 correspond to the target output y =1. Demonstrate the process of learning using Gradient Descent at for 2 iterations to obtain the target 1 for the input 0.5.              \n",
        "\n"
      ],
      "metadata": {
        "id": "TWctjFcg7uIc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gURvbfcO7qwB"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "uWG9dES7SFJW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoidDerivative(x):\n",
        "  return x * (1-x)"
      ],
      "metadata": {
        "id": "uyhx3DWuSMHI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing weights and biases"
      ],
      "metadata": {
        "id": "QX8O_18wSsml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = 1\n",
        "threshold = -0.5"
      ],
      "metadata": {
        "id": "0h02BWZiSub_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input and target output"
      ],
      "metadata": {
        "id": "l_a_gDx7S0Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = 0.5\n",
        "targetOutput = 1"
      ],
      "metadata": {
        "id": "JhZpM0GbS2mY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learning Rate and Iterations"
      ],
      "metadata": {
        "id": "qmCxqchtS8H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learningRate = 0.1\n",
        "iterations = 2"
      ],
      "metadata": {
        "id": "XuVfaceES7c_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for o in range(iterations):\n",
        "\n",
        "    # Forward pass\n",
        "    z1 = w * x + threshold\n",
        "    a1 = sigmoid(z1)\n",
        "\n",
        "    z2 = w * a1 + threshold\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    z3 = w * a2 + threshold\n",
        "    a3 = sigmoid(z3)\n",
        "\n",
        "    z4 = w * a3 + threshold\n",
        "    output = sigmoid(z4)\n",
        "\n",
        "    # Calculating loss\n",
        "    loss = 0.5 * (targetOutput - output) ** 2\n",
        "\n",
        "    # Performing Backpropagation\n",
        "\n",
        "    Output = -(targetOutput - output) * sigmoidDerivative(output)\n",
        "    a3 = Output * w * sigmoidDerivative(a3)\n",
        "    a2 = a3 * w * sigmoidDerivative(a2)\n",
        "    a1 = a2 * w * sigmoidDerivative(a1)\n",
        "\n",
        "    # Updating weights and biases\n",
        "    w -= learningRate * x * a1\n",
        "    threshold -= learningRate * a1\n",
        "\n",
        "    print(f\"Iteration {o + 1}: Output = {output}, Loss = {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfPr7KZgTIgM",
        "outputId": "28f54286-bf58-44d8-a979-f58d2f6d9307"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Output = 0.5000324311632738, Loss = 0.12498378494425329\n",
            "Iteration 2: Output = 0.5001135442852629, Loss = 0.12494323430352092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFinal Weights and Biases:\")\n",
        "print(\"Weight (w):\", w)\n",
        "print(\"Bias (b):\", threshold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPA1uGSNTPim",
        "outputId": "7625eee1-7bf1-42d3-da8b-ede0d155a74e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Weights and Biases:\n",
            "Weight (w): 1.0003907650097232\n",
            "Bias (b): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Apply L1,L2 and elasticNet regularisation to a single-layered neural network with four input nodes and a bias. Assume the input vector has the value [1,2,2,1] and bias = 0 with no activation function at an output node. The learned parameter vector W = [0.25, 0.25, 0.25, 0.25] returns the expected result of \"2.\"\n",
        "\n",
        "  a) Estimate the total loss using L1 , L2 and Elastic Net regularization separately\n",
        "\n",
        "  b)  Compare the effects of L1 , L2 and Elastic Net regularization techniques"
      ],
      "metadata": {
        "id": "zGlGCteAWdGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputVector = np.array([1,2,2,1])\n",
        "threshold = 0\n",
        "expectedResult = 2\n",
        "weights = np.array([0.25,0.25,0.25,0.25])"
      ],
      "metadata": {
        "id": "zl0Tpy0qWynn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss without regularization"
      ],
      "metadata": {
        "id": "6u99ZQWYXB7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = threshold + np.dot(inputVector,weights)\n",
        "lossWithoutRegularization = 0.5 * (prediction - expectedResult ) ** 2"
      ],
      "metadata": {
        "id": "-gb1-H6BW_CR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization Parameters"
      ],
      "metadata": {
        "id": "wBMUsstBXKFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1 = 0.01\n",
        "l2 = 0.01"
      ],
      "metadata": {
        "id": "pe1TjHChXY9e"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1Loss = lossWithoutRegularization + l1 * np.sum(np.abs(weights))\n",
        "l2Loss = lossWithoutRegularization + 0.5 * l2 * np.sum(weights**2)"
      ],
      "metadata": {
        "id": "Cj74rIuaXeVA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elasticNetLoss = lossWithoutRegularization + l1 * np.sum(np.abs(weights))"
      ],
      "metadata": {
        "id": "JfSJG6JkXxxN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loss without Regularization:\", lossWithoutRegularization)\n",
        "print(\"L1 Regularization Loss:\", l1Loss)\n",
        "print(\"L2 Regularization Loss:\", l2Loss)\n",
        "print(\"Elastic Net Regularization Loss:\", elasticNetLoss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M241r0dwX6wa",
        "outputId": "c769c8ef-e70b-4e7b-ca01-cf66ce04d7cf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss without Regularization: 0.125\n",
            "L1 Regularization Loss: 0.135\n",
            "L2 Regularization Loss: 0.12625\n",
            "Elastic Net Regularization Loss: 0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**\n",
        "\n",
        "L1 Regularization Loss and Elastic Net Regularization Loss are higher than the Loss without Regularization. This suggests that the regularization terms are penalizing the model for having non-zero weights. The L2 Regularization Loss is slightly higher than the Loss without Regularization, indicating a penalty for large weights."
      ],
      "metadata": {
        "id": "5il2ah8sYcd9"
      }
    }
  ]
}